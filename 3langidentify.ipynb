{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3langidentify.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO5lIHc2jfzUNuQxNwj6a3U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lopamudrapathak/Vlan.py/blob/main/3langidentify.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mx-9C3xiJ3ly",
        "outputId": "9cedac8b-8f57-448f-af00-1afed6f247ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "EVIVqIPVZvit"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/Dataset.zip -d Dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrtbCSHVLzAn",
        "outputId": "3abdc8f0-5f3d-4b92-e64e-074b02d431c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/Dataset.zip\n",
            "   creating: Dataset/Dataset/\n",
            "  inflating: Dataset/Dataset/1.txt   \n",
            "  inflating: Dataset/Dataset/10.txt  \n",
            "  inflating: Dataset/Dataset/11.txt  \n",
            "  inflating: Dataset/Dataset/12.txt  \n",
            "  inflating: Dataset/Dataset/13.txt  \n",
            "  inflating: Dataset/Dataset/14.txt  \n",
            "  inflating: Dataset/Dataset/15.txt  \n",
            "  inflating: Dataset/Dataset/16.txt  \n",
            "  inflating: Dataset/Dataset/17.txt  \n",
            "  inflating: Dataset/Dataset/2.txt   \n",
            "  inflating: Dataset/Dataset/3.txt   \n",
            "  inflating: Dataset/Dataset/4.txt   \n",
            "  inflating: Dataset/Dataset/5.txt   \n",
            "  inflating: Dataset/Dataset/6.txt   \n",
            "  inflating: Dataset/Dataset/7.txt   \n",
            "  inflating: Dataset/Dataset/8.txt   \n",
            "  inflating: Dataset/Dataset/9.txt   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "old_names_list = [i for i in range(1, 20)]\n",
        "old_to_new = {\n",
        "              'Bhojpuri' : [i for i in range(1,6)], # Files with serial no 1 to 10 are in french, 10 to 20 in English and so on\n",
        "              'Hindi' : [ i for i in range(6,12)],\n",
        "              'Sanskrit' : [i for i in range(12,17)]\n",
        "              }"
      ],
      "metadata": {
        "id": "nuE6I3P4MLHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for lang in old_to_new.keys():\n",
        "    outfile = 'Dataset/' + lang + '.txt'\n",
        "    with open( outfile, 'w') as fname: # destination file i.e., language\n",
        "        for old_file in old_names_list:\n",
        "            if old_file in old_to_new[lang]:\n",
        "                with open('/content/Dataset/Dataset/' + str(old_file) + '.txt') as inp:#opening each match of old_to_new[lang]\n",
        "                    fname.write(inp.read())"
      ],
      "metadata": {
        "id": "lN87PB1FMMEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import  string\n",
        "import numpy as np\n",
        "import math"
      ],
      "metadata": {
        "id": "uOVqwirQSvER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = [2,3,4]  # Here we are choosing bigrams,trigrams and quadgrams; change this value to get n-grams with a particular n\n",
        "k = 50 # Decides how many top n-grams will be used for calculating the distance metric"
      ],
      "metadata": {
        "id": "keBTmTsVTlks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_n_grams(lang, num, k): #returns top k n-grams according to frequency\n",
        "    words = re.sub('['+string.punctuation+']', '', lang) #  punctuation removed\n",
        "    words = words.lower()\n",
        "    words = re.sub('\\s+', ' ', words).strip() # replaces multiple spaces, newline tabs with a single space\n",
        "    words = words.replace(' ','_')# so that we can visualise spaces easily\n",
        "    grams = {}\n",
        "    #print (words)\n",
        "    for i in range(len(words)-num):\n",
        "        temp = words[i:i+num]\n",
        "        if temp in grams:\n",
        "            grams[temp] += 1\n",
        "        else:\n",
        "            grams[temp] = 1\n",
        "    sum_freq = len(words) - num + 1\n",
        "    for key in grams.keys():\n",
        "        red = 1 # reduction factor equal 1 if no '_' is present\n",
        "        if '_' in key: red = 2\n",
        "        grams[key] = round(math.log(grams[key] / (red * sum_freq)), 3) #normalizing by dividing by total no of n-grams for that corpus and taking log                                             \n",
        "    grams = sorted(grams.items(), key= lambda x : x[1], reverse = True) \n",
        "    #print (grams)\n",
        "    final_grams = [] # contains a list of top k n-grams in a given language \n",
        "    log_probs = [] # contains logprobs corresponding to each n-gram\n",
        "    for i in range(len(grams)):\n",
        "        final_grams.append(grams[i][0])\n",
        "        log_probs.append(grams[i][1])\n",
        "    return final_grams, log_probs\n",
        "       \n",
        "with open ('Dataset/Hindi.txt', 'r') as fname:\n",
        "    temp = fname.read()  \n",
        "    create_n_grams(temp, 3, 20) "
      ],
      "metadata": {
        "id": "h8gSRhEaT22y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Zq-6DbjLZhY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lang_list = ['Bhojpuri', 'Hindi', 'Sanskrit']\n",
        "bi_grams = {}\n",
        "tri_grams = {}\n",
        "quad_grams = {}\n",
        "for lang in lang_list:\n",
        "    with open ('Dataset/'+lang+'.txt', 'r') as fname:\n",
        "        file = fname.read()\n",
        "        bi_grams[lang] = create_n_grams(file, n[0], k)\n",
        "        tri_grams[lang] = create_n_grams(file, n[1], k)\n",
        "        quad_grams[lang] = create_n_grams(file, n[2], k)       \n",
        "n_grams = {2 : bi_grams, 3 : tri_grams, 4 : quad_grams}\n",
        "#print (n_grams[2]) "
      ],
      "metadata": {
        "id": "6_ghlG8IUc2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def matching_score_2(test_grams, grams_list, n): # n helps us know whether it is bigram, trigram or quadgram\n",
        "    dist = {lang: 0 for lang in lang_list} # distance corresponding to each language\n",
        "    for gram in test_grams[0]:\n",
        "        for lang in grams_list.keys():\n",
        "            idx_2 = test_grams[0].index(gram)\n",
        "            if gram in n_grams[n][lang][0] : \n",
        "                idx = n_grams[n][lang][0].index(gram)\n",
        "                dist[lang] += abs(n_grams[n][lang][1][idx] - test_grams[1][idx_2])\n",
        "            else: # gram is not present in that language's corpus\n",
        "                dist[lang] += abs(test_grams[1][idx_2])\n",
        "                # penalty term\n",
        "    return dist "
      ],
      "metadata": {
        "id": "NBAWCxVCUvfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def language_identify_2(file_address, st): # argument 'st' denotes whether you are uploading a file or directly copying text\n",
        "    test_bigrams = []\n",
        "    test_trigrams = []\n",
        "    test_quadgrams = []\n",
        "    test_file = []\n",
        "    if st == 'file': # If you are copying a file address\n",
        "        temp = file_address\n",
        "        with open(temp, 'r', errors = 'ignore') as fname: # some characters throw an error with 'utf-8'\n",
        "            file_address = fname.read()\n",
        "    #print (file_address) \n",
        "    test_bigrams = create_n_grams(file_address, 2, k)\n",
        "    test_trigrams = create_n_grams(file_address, 3, k)\n",
        "    test_quadgrams = create_n_grams(file_address, 4, k)\n",
        "    bi_dist = matching_score_2(test_bigrams, bi_grams, 2) \n",
        "    tri_dist = matching_score_2(test_trigrams, tri_grams, 3)\n",
        "    quad_dist = matching_score_2(test_quadgrams, quad_grams, 4) \n",
        "    #print (bi_dist, tri_dist)\n",
        "    final_dist = {}\n",
        "    for lang in bi_dist.keys():\n",
        "        final_dist[lang] =bi_dist[lang] + tri_dist[lang] + quad_dist[lang]\n",
        "    sum_dist = 0\n",
        "    for dist in final_dist.values():\n",
        "        sum_dist += dist\n",
        "    for lang in final_dist.keys():\n",
        "        final_dist[lang] /= sum_dist\n",
        "    dist_list = sorted(final_dist.items(), key= lambda x:x[1])     \n",
        "    #print (dist_list)    \n",
        "    print ('Predicted language :' + dist_list[0][0] + '\\n')\n",
        "    return dist_list[0][0]"
      ],
      "metadata": {
        "id": "c-ckNz1FU1yT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_1 = 'Dataset/lang_test.txt'# CHANGE THIS URL TO INCLUDE YOUR OWN TEST FILE\n",
        "language_identify_2(file_1, 'file')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "8z1PUGSvVEtU",
        "outputId": "5c5e189f-872d-4c55-a97d-dfdd95637ccd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted language :Bhojpuri\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Bhojpuri'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_1 = 'Dataset/lang_test2.txt'# CHANGE THIS URL TO INCLUDE YOUR OWN TEST FILE\n",
        "language_identify_2(file_1, 'file')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "Gwn2U4G8WCN0",
        "outputId": "823d9225-737a-4042-9fe5-b5edad1e63b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted language :Hindi\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hindi'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_1 = 'Dataset/lang_test3.txt'# CHANGE THIS URL TO INCLUDE YOUR OWN TEST FILE\n",
        "language_identify_2(file_1, 'file')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "ycRZPYjTWeYE",
        "outputId": "e2494b12-03f2-44b8-b0a8-c53f884a0977"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted language :Hindi\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hindi'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_1 = 'Dataset/lang_test4.txt'# CHANGE THIS URL TO INCLUDE YOUR OWN TEST FILE\n",
        "language_identify_2(file_1, 'file')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "vmWwtoj-XGrh",
        "outputId": "f4418723-d389-4845-876f-15c124fcbecf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted language :Bhojpuri\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Bhojpuri'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "while (True):\n",
        "    test_input = input(\"\\n Enter few lines of text: \")\n",
        "    language_identify_2(test_input, 'not_file')\n",
        "    user = input(\"Do you want to test more? (y/Y or N/n)\")\n",
        "    if user == 'N' or user == 'n':\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFjAcjcgZ1Kh",
        "outputId": "03c6743d-2adc-4a4c-b1e4-81b34b53e190"
      },
      "execution_count": 39,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Enter few lines of text: gdysysy\n",
            "Predicted language :Bhojpuri\n",
            "\n",
            "Do you want to test more? (y/Y or N/n)n\n"
          ]
        }
      ]
    }
  ]
}